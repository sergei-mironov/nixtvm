Useful articles
===============

General
-------

### Books

* http://neuralnetworksanddeeplearning.com/
  2015, Neural Networks and Deep Learning book
* https://github.com/janishar/mit-deep-learning-book-pdf/tree/master/complete-book-bookmarked-pdf
  2017, MIT Deep Learning Book

### Datasets

* http://yann.lecun.com/exdb/mnist/
  ?, MNIST
* https://www.cs.toronto.edu/~kriz/cifar.html
  ?, CIFAR-10, CIFAR-100
* https://catalog.ldc.upenn.edu/ldc99t42
  ?, Penn Treebank v3
* https://www.microsoft.com/en-us/research/publication/mctest-challenge-dataset-open-domain-machine-comprehension-text/
  2013, MCTest
* https://nlp.stanford.edu/projects/glove/
  2014, GloVe: Global Vectors for Word Representation
* SQuAD, The Standford Question Answering Dataset
  2018, https://rajpurkar.github.io/SQuAD-explorer/

### Precision problem

Media:

* https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/
  2014, Exp-normalize trick
* https://benjaminjurke.com/content/articles/2015/loss-of-significance-in-floating-point-computations/
  2015, Jurke, Analyzing the loss of significance in floating-point computations

Articles:

* https://arxiv.org/pdf/1805.01078
  2018, Exploration of Numerical Precision in Deep Neural Networks (some problems with downloading)

### Quantizing

* https://arxiv.org/pdf/1611.10176.pdf <br/>
  2016, Effective Quantization Methods for Recurrent Neural Networks
* https://arxiv.org/pdf/1712.05877.pdf <br/>
  2017, Quantization and Training of Neural Networks for Efficient
  Integer-Arithmetic-Only Inference
* https://arxiv.org/pdf/1806.08342.pdf <br/>
  2018, Quantizing deep convolutional networks for efficient inference: A whitepaper
* https://arxiv.org/ftp/arxiv/papers/1802/1802.02615.pdf <br/>
  2018, Effective Quantization Approaches for Recurrent Neural Networks

Tasks by classes
----------------

### CV

* VGG
  - https://arxiv.org/abs/1409.1556
    2014, Very Deep Convolutional Networks for Large-Scale Image Recognition
  - https://www.cs.toronto.edu/~frossard/post/vgg16/
    Blog, Model and pre-trained parameters for VGG16 in TensorFlow

* Inception
  - https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202
  - https://arxiv.org/pdf/1409.4842v1.pdf
    2014, Going deeper with convolution

* AlexNet ??
  - ??

* Squeezenet
  - https://arxiv.org/pdf/1602.07360
    2016, SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size
  - https://en.wikipedia.org/wiki/SqueezeNet
    2016, Wikipedia
  - https://github.com/DeepScale/SqueezeNet
    2016, GitHub repository

* Densnet
  - https://arxiv.org/pdf/1608.06993v3.pdf
    2016, Densely Connected Convolutional Networks
  - https://towardsdatascience.com/densenet-2810936aeebb
    2017, Article

* Mobile Net V1
  - https://arxiv.org/abs/1704.04861
    2017, MobileNet V1 article
  - https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md
    TensorFlow README

* Mobile Net V2
  - https://arxiv.org/abs/1801.04381
    2018, MobileNet V2 article: Inverted Residuals and Linear Bottlenecks
  - https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet
    TensorFlow README



### NLP

General Media:

* https://aclanthology.coli.uni-saarland.de/
  Study of computational linguistics and natural language processing

Huawei Media:

* https://pdfs.semanticscholar.org/presentation/51d9/81c1b28818fd0ee94dd3e607e1004874dfef.pdf
  2015, Research on Deep Learning for Natural Language Processing at Huawei Noahâ€™s Ark Lab
* https://www.huawei.com/en/about-huawei/publications/winwin-magazine/AI/intelligent-agents-tomorrow-digital-valets
  2016, Nuawei NLP news
* http://www.aclweb.org/anthology/N16-4004
  Noah Ark document

Articles:

* https://arxiv.org/pdf/1506.02078.pdf
  2015, Visualizing and Understanding recurrent Networks
* https://arxiv.org/pdf/1706.03762.pdf
  2017, Attention Is All You Need
* https://arxiv.org/pdf/1708.02709.pdf
  2018, Recent Trends in Deep Learning Based Natural Language Processing
* http://aclweb.org/anthology/D15-1141
  2018, Long Short-Term Memory Neural Networks for Chinese Word Segmentation

### QA

* https://rajpurkar.github.io/SQuAD-explorer/
  - The Stanford Question Answering DataSet and Leaderboard


* BiDAF, Bidirectional Attention Flow for Machine Comprehension
  - 2014, https://arxiv.org/pdf/1611.01603v6
  - [Homepage](https://allenai.github.io/bi-att-flow/)
  - https://github.com/allenai/bi-att-flow
  - https://github.com/galsang/BiDAF-pytorch

* BERT Model
  - https://arxiv.org/pdf/1810.04805.pdf
    2018, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
  - https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
    2018, BERT explained

### OCR

* https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=726791
  1998, LeCun, Gradient-Based Learning Applied to Document Recognition
* https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6628705
  2013, High-Performance OCR for Printed English and Fraktur using LSTM Networks
  - https://sourceforge.net/projects/rnnl/
    RNNLib - OpenSource library which was used by the authors.
* https://arxiv.org/pdf/1508.02774.pdf
  2015, Bruel, Benchmarking of LSTM Networks
* https://hackernoon.com/latest-deep-learning-ocr-with-keras-and-supervisely-in-15-minutes-34aecd630ed8
  2017, some Optical Character Recognition state-of-the-art article.
* https://arxiv.org/pdf/1805.09441.pdf
  2018, Implicit Language Model in LSTM for OCR


### ASR

Media

* https://www.slideshare.net/ssusercd5833/sequence-learning-with-ctc-technique
  Chun Hao Wang, Slides with lots of formuals

Articles

* https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN_ASLPTrans2-14.pdf
  2014, Abdel-Hamid, Convolutional Neural Networks for Speech Recognition
* http://proceedings.mlr.press/v32/graves14.pdf
  2014, Graves, Towards End-to-End Speech Recognition with Recurrent Neural Networks
* http://homepages.inf.ed.ac.uk/llu/pdf/llu_icassp16.pdf
  2016, Lu, On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition
* https://arxiv.org/pdf/1610.09975.pdf
  2016, Soltau, Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition
* https://arxiv.org/pdf/1705.10874.pdf
  2017, Zhang, Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments


### AD

Media

* http://vertex.ai/blog/fully-automatic-differentiation
* https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/
  2013, Introduction to Automatic Differentiation
* http://www.columbia.edu/~ahd2125/post/2015/12/5/
  2015, Automatic Differentiation or Mathemagically Finding Derivatives (blog, 2015). !Errors
* http://www.autodiff.org/?module=Introduction&submenu=Selected%20Books
  Collection of textbooks on AD
* https://autodiff-workshop.github.io/
  2017, Autodiff Workshop

Articles

* http://www.bcl.hamilton.ie/~qobi/stalingrad/
  2005, Reverse-Mode AD in a Functional Framework: Lambda the Ultimate Backpropagator
* http://conal.net/papers/beautiful-differentiation/
  Forward-mode AD in Haskell, vector spaces
* https://arxiv.org/pdf/1711.01348
  2017, Automatic differentiation for tensor algebras, tech.report
* https://arxiv.org/pdf/1806.02136.pdf
  2018, Peyton Jones, Efficient Differentiable Programming in a Functional Array-Processing Language
* https://people.csail.mit.edu/tzumao/gradient_halide/gradient_halide.pdf
  2018, Differentiable Programming for Image Processing and Deep Learning in Halide
* https://arxiv.org/pdf/1803.10228.pdf
  2018, Demystifying Differentiable Programming: Shift/Reset the Penultimate Backpropagator
* http://conal.net/papers/essence-of-ad/
  2018, The simple essence of automatic differentiation, by Elliott, at ICFP 2018



Models by classes
-----------------

### CNN

Media:

* https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25
  Dilated convolution
* https://www.oreilly.com/ideas/visualizing-convolutional-neural-networks
  2017, Visualizing convolutional neural networks
* https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1
  2018, Shafkat, Intuitively Understanding Convolutions for Deep Learning

Articles:

* https://arxiv.org/pdf/1505.00387.pdf
  2015, Highway Networks

### RNN

* https://arxiv.org/pdf/1409.2329.pdf
  2015, Zaremba, Recurrent Neural Network Regularization

* https://arxiv.org/abs/1810.04719
  2018, Fully Supervised Speaker Diarization
  - [https://github.com/google/uis-rnn](GitHub)

### Memory Networks

* https://arxiv.org/abs/1410.3916
  2014, Memory Networks

### LSTM

Media:

* https://www.quora.com/What-is-the-best-research-paper-about-recurrent-neural-networks-to-start-with
  What is the best research paper about recurrent neural networks to start with?
* http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
  LSTM tutorial
* https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4
  2015, Colah's blog
* http://colah.github.io/posts/2015-08-Understanding-LSTMs/
  Understanding LSTM networks
* https://en.wikipedia.org/wiki/Differentiable_neural_computer
  WIKI: Differentiable memory computer

Articles:

* http://www.bioinf.jku.at/publications/older/2604.pdf <br/>
  1997, Long Short-Term Memory
* http://www.felixgers.de/papers/phd.pdf <br/>
  2001, Long Short-Term Memory in Recurrent Neural Networks, PhD thesis
* https://arxiv.org/pdf/1508.02774.pdf <br/>
  2015, Bruel, Benchmarking of LSTM Networks
* https://arxiv.org/pdf/1503.04069.pdf <br/>
  2017, LSTM: A Search Space Odyssey
* https://arxiv.org/ftp/arxiv/papers/1802/1802.02615.pdf <br/>
  2018(?), Effective Quantization Approaches for Recurrent Neural Networks


TVM
---

### Integration

* https://tvm.ai/2018/03/23/nmt-transformer-optimize.html
  2018, TensorFlow integration

* https://tvm.ai/2018/08/10/DLPack-Bridge.html
  2018, PyTorch integration

### Autotuner

* https://arxiv.org/pdf/1805.08166.pdf
  Learning to Optimize Tensor Programs

### General

* https://arxiv.org/pdf/1802.04799.pdf
  TVM: An Automated End-to-End Optimizing Compiler for Deep Learning

* https://github.com/dmlc/dmlc.github.io/blob/master/\_posts/2016-09-29-build-your-own-tensorflow-with-nnvm-and-torch.markdown
  How about build your own TensorFlow with NNVM and Torch7

* https://github.com/andersy005/tvm-in-action

* https://arxiv.org/pdf/1807.04188.pdf
  2018, VTA: An Open Hardware-Software Stack for Deep Learning

* https://dl.acm.org/ft_gateway.cfm?id=3211348&ftid=1979148&dwn=1&CFID=8566367&CFTOKEN=e89fe1de14e82d30-9B3A947F-BA70-A2B8-C972CEE81188C1C5
  2018, Relay: A New IR for Machine Learning Frameworks

### Competitors

* http://halide-lang.org/
  Halide itself

* https://www.tensorflow.org/performance/xla/
  XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear
  algebra that optimizes TensorFlow computations.
  - https://haosdent.gitbooks.io/tensorflow-document/content/resources/xla_prerelease.html

* https://github.com/facebookresearch/TensorComprehensions
  A domain specific language to express machine learning workloads.
  - https://arxiv.org/abs/1802.04730
    Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions

* https://github.com/plaidml/plaidml
  PlaidML - PlaidML is the easiest, fastest way to learn and deploy deep
  learning on any device, especially those running macOS or Windows.

* http://dlvm.org/
  - https://arxiv.org/pdf/1711.03016
    DLVM: A modern compiler infrastructure for deep learning systems

* https://github.com/vgvassilev/clad
  - https://llvm.org/devmtg/2013-11/slides/Vassilev-Poster.pdf
    clad - Automatic Differentiation using Clang

* http://ngraph.nervanasys.com/docs/latest/
  nGraph (Intel, business with PlaidML)

* https://software.intel.com/en-us/openvino-toolkit
  OpenVINO (Intel)

* https://arxiv.org/pdf/1804.10694.pdf
  2018, Tiramisu: A Code Optimization Framework for High Performance Systems

* https://github.com/xiaomi/mace
  2018, MACE is a deep learning inference framework optimized for mobile heterogeneous computing platforms

* https://github.com/Tencent/PocketFlow
  PocketFlow, a kind of Autotuner for TF?

### Benchmarks

* https://github.com/dmlc/tvm/wiki/Benchmark
  TVM Benchmarking WIKI (remote devices for now)

* http://vertex.ai/blog/compiler-comparison
  [By PlaidML] Comparision between PlaidML, TVM, TensorComprehensions

* https://github.com/plaidml/plaidbench/tree/tensorcomp
  [By PlaidML] Benchmarks for Keras kernels, compares TVM and TC

* https://github.com/u39kun/deep-learning-benchmark

* https://knowm.org/deep-learning-frameworks-hands-on-review/
  General ML frameworks Review

### Related

* https://arxiv.org/pdf/1805.00907.pdf
  Glow: Graph Lowering Compiler Techniques for Neural Networks

* https://en.wikipedia.org/wiki/Polytope\_model

TensorFlow
----------

Media:

* https://github.com/chiphuyen/stanford-tensorflow-tutorials
* https://github.com/aymericdamien/TensorFlow-Examples/
* https://github.com/adventuresinML/adventures-in-ml-code
* https://github.com/philipperemy/tensorflow-multi-dimensional-lstm

### Profiling

* https://towardsdatascience.com/howto-profile-tensorflow-1a49fb18073d
  2017, HowTo profile TensorFlow


